{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Picking Out the Best Classification Model for the Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainPath = '../data/hateval2019_en_train_clean.csv'\n",
    "testPath = '../data/hateval2019_en_test_clean.csv'\n",
    "\n",
    "trainSet = pd.read_csv(trainPath)\n",
    "testSet = pd.read_csv(testPath)\n",
    "hateSet = trainSet[trainSet['HS']==1]\n",
    "\n",
    "classifiers = ['Multinomial NB', 'Bernoulli NB', 'Gaussian NB', 'Logistic Regression ', 'Stochastic Gradient Descent', 'Support Vector Machine', 'Random Forest', 'Gradient Boosting']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifying(x, x_test, y, y_test):\n",
    "    lr = LogisticRegression(max_iter=10000)\n",
    "    sgd = SGDClassifier()\n",
    "    multi_nb = MultinomialNB()\n",
    "    gaussian_nb = GaussianNB()\n",
    "    bernoulli_nb = BernoulliNB()\n",
    "    svm = SVC()\n",
    "    rf = RandomForestClassifier(n_estimators = 1000)\n",
    "    gb = GradientBoostingClassifier(n_estimators=1000, learning_rate=1.0,max_depth=1, random_state=0)\n",
    "\n",
    "    %time multi_nb.fit(x, y)\n",
    "    %time bernoulli_nb.fit(x, y)\n",
    "    %time gaussian_nb.fit(x.toarray(), y)\n",
    "    %time lr.fit(x, y)\n",
    "    %time sgd.fit(x, y)\n",
    "    %time svm.fit(x, y)\n",
    "    %time rf.fit(x, y)\n",
    "    %time gb.fit(x, y)\n",
    "    \n",
    "    accuracy_array = predicting(x_test, y_test, multi_nb, bernoulli_nb, gaussian_nb, lr, sgd, svm, rf, gb)\n",
    "    f1_array = metric_f1(x_test, y_test, multi_nb, bernoulli_nb, gaussian_nb, lr, sgd, svm, rf, gb)\n",
    "    \n",
    "    return accuracy_array, f1_array\n",
    "\n",
    "def predicting(x, y, multi_nb, bernoulli_nb, gaussian_nb, lr, sgd, svm, rf, gb):\n",
    "    #Naive Bayes\n",
    "    y_pred_class_multi_nb = multi_nb.predict(x)\n",
    "    y_pred_class_bernoulli_nb = bernoulli_nb.predict(x)\n",
    "    y_pred_class_gaussian_nb = gaussian_nb.predict(x.toarray())\n",
    "\n",
    "    multi_nb_acc = metrics.accuracy_score(y, y_pred_class_multi_nb)\n",
    "    bernoulli_nb_acc = metrics.accuracy_score(y, y_pred_class_bernoulli_nb)\n",
    "    gaussian_nb_acc = metrics.accuracy_score(y, y_pred_class_gaussian_nb)\n",
    "\n",
    "    #Linear Models\n",
    "    y_pred_class_lr = lr.predict(x)\n",
    "    y_pred_class_sgd = sgd.predict(x)\n",
    "\n",
    "    lr_acc = metrics.accuracy_score(y, y_pred_class_lr)\n",
    "    sgd_acc = metrics.accuracy_score(y, y_pred_class_sgd)\n",
    "\n",
    "\n",
    "    #Support Vector Machine\n",
    "    y_pred_class_svm = svm.predict(x)\n",
    "\n",
    "    svm_acc = metrics.accuracy_score(y, y_pred_class_svm)\n",
    "\n",
    "    #Ensemble\n",
    "    y_pred_class_rf = rf.predict(x)\n",
    "    y_pred_class_gb = gb.predict(x)\n",
    "\n",
    "    rf_acc = metrics.accuracy_score(y, y_pred_class_rf)\n",
    "    gb_acc = metrics.accuracy_score(y, y_pred_class_gb)\n",
    "    \n",
    "    return [multi_nb_acc, bernoulli_nb_acc, gaussian_nb_acc, lr_acc, sgd_acc, svm_acc, rf_acc, gb_acc]\n",
    "\n",
    "def metric_f1(x,y, multi_nb, bernoulli_nb, gaussian_nb, lr, sgd, svm, rf, gb):\n",
    "    #Naive Bayes\n",
    "    y_pred_class_multi_nb = multi_nb.predict(x)\n",
    "    y_pred_class_bernoulli_nb = bernoulli_nb.predict(x)\n",
    "    y_pred_class_gaussian_nb = gaussian_nb.predict(x.toarray())\n",
    "\n",
    "    multi_nb_acc = f1_score(y, y_pred_class_multi_nb)\n",
    "    bernoulli_nb_acc = f1_score(y, y_pred_class_bernoulli_nb)\n",
    "    gaussian_nb_acc = f1_score(y, y_pred_class_gaussian_nb)\n",
    "\n",
    "    #Linear Models\n",
    "    y_pred_class_lr = lr.predict(x)\n",
    "    y_pred_class_sgd = sgd.predict(x)\n",
    "\n",
    "    lr_acc = f1_score(y, y_pred_class_lr)\n",
    "    sgd_acc = f1_score(y, y_pred_class_sgd)\n",
    "\n",
    "\n",
    "    #Support Vector Machine\n",
    "    y_pred_class_svm = svm.predict(x)\n",
    "\n",
    "    svm_acc = f1_score(y, y_pred_class_svm)\n",
    "\n",
    "    #Ensemble\n",
    "    y_pred_class_rf = rf.predict(x)\n",
    "    y_pred_class_gb = gb.predict(x)\n",
    "\n",
    "    rf_acc = f1_score(y, y_pred_class_rf)\n",
    "    gb_acc = f1_score(y, y_pred_class_gb)\n",
    "    \n",
    "    return [multi_nb_acc, bernoulli_nb_acc, gaussian_nb_acc, lr_acc, sgd_acc, svm_acc, rf_acc, gb_acc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vect = CountVectorizer(stop_words='english', ngram_range=(1, 2), min_df=2, max_df=0.5)\n",
    "#vect = TfidfVectorizer(stop_words='english', ngram_range=(1, 1), min_df=4, max_df=0.5)\n",
    "vect = CountVectorizer(stop_words='english', ngram_range=(1, 2), min_df=2)\n",
    "\n",
    "x_train_dtm = vect.fit_transform(trainSet.text)\n",
    "x_test_dtm = vect.transform(testSet.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3 ms\n",
      "Wall time: 4 ms\n",
      "Wall time: 2.64 s\n",
      "Wall time: 453 ms\n",
      "Wall time: 40 ms\n",
      "Wall time: 8.35 s\n",
      "Wall time: 1min 16s\n",
      "Wall time: 6.74 s\n"
     ]
    }
   ],
   "source": [
    "#Hate Score\n",
    "accuracy_array, f1_array = classifying(x_train_dtm, x_test_dtm, trainSet.HS, testSet.HS)\n",
    "\n",
    "data = pd.DataFrame(accuracy_array, columns=['Accuracy'], index=classifiers)\n",
    "f1 = pd.DataFrame(f1_array, columns=['f1'], index=classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             Accuracy\n",
      "Multinomial NB               0.461333\n",
      "Bernoulli NB                 0.490667\n",
      "Gaussian NB                  0.459667\n",
      "Logistic Regression          0.493000\n",
      "Stochastic Gradient Descent  0.492333\n",
      "Support Vector Machine       0.459000\n",
      "Random Forest                0.450000\n",
      "Gradient Boosting            0.481333\n",
      "\n",
      "\n",
      "                                   f1\n",
      "Multinomial NB               0.598609\n",
      "Bernoulli NB                 0.602083\n",
      "Gaussian NB                  0.585528\n",
      "Logistic Regression          0.606061\n",
      "Stochastic Gradient Descent  0.601622\n",
      "Support Vector Machine       0.589426\n",
      "Random Forest                0.594595\n",
      "Gradient Boosting            0.602656\n"
     ]
    }
   ],
   "source": [
    "print(data)\n",
    "print(\"\\n\")\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vect = TfidfVectorizer(stop_words='english', ngram_range=(1, 1), min_df=4, max_df=0.5)\n",
    "x_train_dtm = vect.fit_transform(hateSet.text)\n",
    "x_test_dtm = vect.transform(testSet.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2 ms\n",
      "Wall time: 2 ms\n",
      "Wall time: 471 ms\n",
      "Wall time: 100 ms\n",
      "Wall time: 10 ms\n",
      "Wall time: 1.16 s\n",
      "Wall time: 15.2 s\n",
      "Wall time: 2.47 s\n"
     ]
    }
   ],
   "source": [
    "#Target Score\n",
    "#accuracy_array, f1_array = classifying(x_train_dtm, x_test_dtm, trainSet.TR, testSet.TR)\n",
    "accuracy_array, f1_array = classifying(x_train_dtm, x_test_dtm, hateSet.TR, testSet.TR)\n",
    "\n",
    "data = pd.DataFrame(accuracy_array, columns=['Accuracy'], index=classifiers)\n",
    "f1 = pd.DataFrame(f1_array, columns=['f1'], index=classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             Accuracy\n",
      "Multinomial NB               0.663000\n",
      "Bernoulli NB                 0.652000\n",
      "Gaussian NB                  0.658000\n",
      "Logistic Regression          0.690667\n",
      "Stochastic Gradient Descent  0.664333\n",
      "Support Vector Machine       0.681667\n",
      "Random Forest                0.674000\n",
      "Gradient Boosting            0.671667\n",
      "\n",
      "\n",
      "                                   f1\n",
      "Multinomial NB               0.500247\n",
      "Bernoulli NB                 0.493204\n",
      "Gaussian NB                  0.459431\n",
      "Logistic Regression          0.506383\n",
      "Stochastic Gradient Descent  0.470836\n",
      "Support Vector Machine       0.513500\n",
      "Random Forest                0.508048\n",
      "Gradient Boosting            0.498217\n"
     ]
    }
   ],
   "source": [
    "print(data)\n",
    "print(\"\\n\")\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2 ms\n",
      "Wall time: 2 ms\n",
      "Wall time: 476 ms\n",
      "Wall time: 151 ms\n",
      "Wall time: 12 ms\n",
      "Wall time: 1.67 s\n",
      "Wall time: 19.9 s\n",
      "Wall time: 2.59 s\n"
     ]
    }
   ],
   "source": [
    "#Aggressive Score\n",
    "#accuracy_array, f1_array = classifying(x_train_dtm, x_test_dtm, trainSet.AG, testSet.AG)\n",
    "accuracy_array, f1_array = classifying(x_train_dtm, x_test_dtm, hateSet.AG, testSet.TR)\n",
    "\n",
    "data = pd.DataFrame(accuracy_array, columns=['Accuracy'], index=classifiers)\n",
    "f1 = pd.DataFrame(f1_array, columns=['f1'], index=classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             Accuracy\n",
      "Multinomial NB               0.421667\n",
      "Bernoulli NB                 0.524333\n",
      "Gaussian NB                  0.354667\n",
      "Logistic Regression          0.522667\n",
      "Stochastic Gradient Descent  0.516000\n",
      "Support Vector Machine       0.500333\n",
      "Random Forest                0.451333\n",
      "Gradient Boosting            0.510333\n",
      "\n",
      "\n",
      "                                   f1\n",
      "Multinomial NB               0.060639\n",
      "Bernoulli NB                 0.028591\n",
      "Gaussian NB                  0.237195\n",
      "Logistic Regression          0.113861\n",
      "Stochastic Gradient Descent  0.131579\n",
      "Support Vector Machine       0.049461\n",
      "Random Forest                0.058352\n",
      "Gradient Boosting            0.111313\n"
     ]
    }
   ],
   "source": [
    "print(data)\n",
    "print(\"\\n\")\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "\n",
    "## Hate Score\n",
    "```\n",
    "                             Accuracy\n",
    "Multinomial NB               0.507000\n",
    "Bernoulli NB                 0.473667\n",
    "Gaussian NB                  0.461667\n",
    "Logistic Regression          0.488000\n",
    "Stochastic Gradient Descent  0.491333\n",
    "Support Vector Machine       0.479667\n",
    "Random Forest                0.454667\n",
    "Gradient Boosting            0.467667\n",
    "\n",
    "\n",
    "                                   f1\n",
    "Multinomial NB               0.605705\n",
    "Bernoulli NB                 0.605349\n",
    "Gaussian NB                  0.589581\n",
    "Logistic Regression          0.606960\n",
    "Stochastic Gradient Descent  0.607106\n",
    "Support Vector Machine       0.603908\n",
    "Random Forest                0.597837\n",
    "Gradient Boosting            0.592914\n",
    "\n",
    "```\n",
    "\n",
    "## Target Score\n",
    "\n",
    "### No Filter\n",
    "```\n",
    "Accuracy\n",
    "                             Accuracy\n",
    "Multinomial NB               0.832000\n",
    "Bernoulli NB                 0.700000\n",
    "Gaussian NB                  0.650667\n",
    "Logistic Regression          0.800667\n",
    "Stochastic Gradient Descent  0.768333\n",
    "Support Vector Machine       0.775333\n",
    "Random Forest                0.740667\n",
    "Gradient Boosting            0.821000\n",
    "\n",
    "\n",
    "                                   f1\n",
    "Multinomial NB               0.408451\n",
    "Bernoulli NB                 0.524815\n",
    "Gaussian NB                  0.424176\n",
    "Logistic Regression          0.544901\n",
    "Stochastic Gradient Descent  0.541254\n",
    "Support Vector Machine       0.546433\n",
    "Random Forest                0.524450\n",
    "Gradient Boosting            0.584043\n",
    "\n",
    "```\n",
    "\n",
    "### Filter HS=1\n",
    "```\n",
    "                             Accuracy\n",
    "Multinomial NB               0.707000\n",
    "Bernoulli NB                 0.654333\n",
    "Gaussian NB                  0.588000\n",
    "Logistic Regression          0.711333\n",
    "Stochastic Gradient Descent  0.698667\n",
    "Support Vector Machine       0.695333\n",
    "Random Forest                0.669000\n",
    "Gradient Boosting            0.677333\n",
    "\n",
    "\n",
    "                                   f1\n",
    "Multinomial NB               0.525634\n",
    "Bernoulli NB                 0.494886\n",
    "Gaussian NB                  0.428307\n",
    "Logistic Regression          0.527293\n",
    "Stochastic Gradient Descent  0.503297\n",
    "Support Vector Machine       0.518947\n",
    "Random Forest                0.502754\n",
    "Gradient Boosting            0.509128\n",
    "```\n",
    "\n",
    "## Aggressive Score\n",
    "\n",
    "### No Filter\n",
    "```\n",
    "                             Accuracy\n",
    "Multinomial NB               0.744333\n",
    "Bernoulli NB                 0.639667\n",
    "Gaussian NB                  0.406000\n",
    "Logistic Regression          0.746000\n",
    "Stochastic Gradient Descent  0.694000\n",
    "Support Vector Machine       0.708667\n",
    "Random Forest                0.662000\n",
    "Gradient Boosting            0.639333\n",
    "\n",
    "\n",
    "                                   f1\n",
    "Multinomial NB               0.270219\n",
    "Bernoulli NB                 0.354627\n",
    "Gaussian NB                  0.332084\n",
    "Logistic Regression          0.331579\n",
    "Stochastic Gradient Descent  0.337662\n",
    "Support Vector Machine       0.311811\n",
    "Random Forest                0.335518\n",
    "Gradient Boosting            0.281541\n",
    "```\n",
    "\n",
    "### Filter HS=1\n",
    "```\n",
    "                             Accuracy\n",
    "Multinomial NB               0.464333\n",
    "Bernoulli NB                 0.454667\n",
    "Gaussian NB                  0.331000\n",
    "Logistic Regression          0.482667\n",
    "Stochastic Gradient Descent  0.474000\n",
    "Support Vector Machine       0.483333\n",
    "Random Forest                0.438667\n",
    "Gradient Boosting            0.500000\n",
    "\n",
    "\n",
    "                                   f1\n",
    "Multinomial NB               0.041741\n",
    "Bernoulli NB                 0.029656\n",
    "Gaussian NB                  0.231329\n",
    "Logistic Regression          0.065060\n",
    "Stochastic Gradient Descent  0.119420\n",
    "Support Vector Machine       0.069628\n",
    "Random Forest                0.069613\n",
    "Gradient Boosting            0.106079\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
